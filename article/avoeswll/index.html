<!doctype html><html lang="zh-CN"><head><meta charset="utf-8" /><meta name="viewport" content="width=device-width,initial-scale=1" /><meta name="generator" content="VuePress 2.0.0-rc.24" /><meta name="theme" content="VuePress Theme Plume 1.0.0-rc.162" /><script id="check-mac-os">document.documentElement.classList.toggle('mac', /Mac|iPhone|iPod|iPad/i.test(navigator.platform))</script><script id="check-dark-mode">;(function () {const um= localStorage.getItem('vuepress-theme-appearance') || 'auto';const sm = window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches;const isDark = um === 'dark' || (um !== 'light' && sm);document.documentElement.dataset.theme = isDark ? 'dark' : 'light';})();</script><title>第2章 生成式模型（LLM） | 面试造飞机-九折技术</title><meta name="description" content="ai,后端,架构,软件开发"><link rel="preload" href="/assets/style-FHgMeouG.css" as="style"><link rel="stylesheet" href="/assets/style-FHgMeouG.css"><link rel="modulepreload" href="/assets/app-zSjPkmgN.js"><link rel="modulepreload" href="/assets/index.html-9rBiEy0v.js"><link rel="prefetch" href="/assets/index.html-CruQpEFZ.js" as="script"><link rel="prefetch" href="/assets/index.html-C3juNrAA.js" as="script"><link rel="prefetch" href="/assets/index.html-WrrmR-U6.js" as="script"><link rel="prefetch" href="/assets/index.html-BxKeGKpB.js" as="script"><link rel="prefetch" href="/assets/index.html-CVFnxwcH.js" as="script"><link rel="prefetch" href="/assets/index.html-DyFE7D4j.js" as="script"><link rel="prefetch" href="/assets/index.html-CU-jv3Kb.js" as="script"><link rel="prefetch" href="/assets/index.html-DtoVbmm-.js" as="script"><link rel="prefetch" href="/assets/index.html-CSuHdEn8.js" as="script"><link rel="prefetch" href="/assets/index.html-B03GQmlV.js" as="script"><link rel="prefetch" href="/assets/index.html-C_ujFDxu.js" as="script"><link rel="prefetch" href="/assets/index.html-D9z3Kqwm.js" as="script"><link rel="prefetch" href="/assets/index.html-joW4Z0Yz.js" as="script"><link rel="prefetch" href="/assets/index.html-ha9O2sKE.js" as="script"><link rel="prefetch" href="/assets/index.html-C6djjmAd.js" as="script"><link rel="prefetch" href="/assets/index.html-BrcRTbiv.js" as="script"><link rel="prefetch" href="/assets/index.html-BzJwyU71.js" as="script"><link rel="prefetch" href="/assets/index.html-CO5JHRFv.js" as="script"><link rel="prefetch" href="/assets/404.html-DomZmev8.js" as="script"><link rel="prefetch" href="/assets/index.html-CruQpEFZ.js" as="script"><link rel="prefetch" href="/assets/index.html-BogfM7G9.js" as="script"><link rel="prefetch" href="/assets/index.html-D2dpQlUH.js" as="script"><link rel="prefetch" href="/assets/index.html-ChsxAPN0.js" as="script"><link rel="prefetch" href="/assets/photoswipe.esm-CKV1Bsxh.js" as="script"><link rel="prefetch" href="/assets/searchBox-default-U0PWlw1-.js" as="script"><link rel="prefetch" href="/assets/SearchBox-kfhR2hJN.js" as="script"></head><body><div id="app"><!--[--><!--[--><div class="theme-plume vp-layout" vp-container data-v-f306e958><!--[--><!--[--><!--]--><!--[--><span tabindex="-1" data-v-b503e7b0></span><a href="#VPContent" class="vp-skip-link visually-hidden" data-v-b503e7b0> Skip to content </a><!--]--><!----><header class="vp-nav" data-v-f306e958 data-v-5271e5e5><div class="vp-navbar" vp-navbar data-v-5271e5e5 data-v-f21177fa><div class="wrapper" data-v-f21177fa><div class="container" data-v-f21177fa><div class="title" data-v-f21177fa><div class="vp-navbar-title" data-v-f21177fa data-v-15b374d8><a class="vp-link link no-icon title" href="/" data-v-15b374d8><!--[--><!--[--><!--]--><!----><span data-v-15b374d8>面试造飞机-九折技术</span><!--[--><!--]--><!--]--><!----></a></div></div><div class="content" data-v-f21177fa><div class="content-body" data-v-f21177fa><!--[--><!--]--><div class="vp-navbar-search search" data-v-f21177fa><div class="search-wrapper" data-v-bc840d77><!----><div id="local-search" data-v-bc840d77><button type="button" class="mini-search mini-search-button" aria-label="搜索文档" data-v-bc840d77><span class="mini-search-button-container"><span class="mini-search-search-icon vpi-mini-search" aria-label="search icon"></span><span class="mini-search-button-placeholder">搜索文档</span></span><span class="mini-search-button-keys"><kbd class="mini-search-button-key"></kbd><kbd class="mini-search-button-key">K</kbd></span></button></div></div></div><!--[--><!--]--><nav aria-labelledby="main-nav-aria-label" class="vp-navbar-menu menu" data-v-f21177fa data-v-43aff82f><span id="main-nav-aria-label" class="visually-hidden" data-v-43aff82f>Main Navigation</span><!--[--><!--[--><a class="vp-link link navbar-menu-link" href="/" tabindex="0" data-v-43aff82f data-v-4bb27b21><!--[--><!----><span data-v-4bb27b21>首页</span><!----><!--]--><!----></a><!--]--><!--[--><a class="vp-link link navbar-menu-link" href="/ai/" tabindex="0" data-v-43aff82f data-v-4bb27b21><!--[--><!----><span data-v-4bb27b21>AI技术</span><!----><!--]--><!----></a><!--]--><!--[--><a class="vp-link link navbar-menu-link" href="/backend/" tabindex="0" data-v-43aff82f data-v-4bb27b21><!--[--><!----><span data-v-4bb27b21>后端技术</span><!----><!--]--><!----></a><!--]--><!--[--><a class="vp-link link navbar-menu-link" href="/architecture/" tabindex="0" data-v-43aff82f data-v-4bb27b21><!--[--><!----><span data-v-4bb27b21>架构设计</span><!----><!--]--><!----></a><!--]--><!--[--><a class="vp-link link navbar-menu-link" href="/about/" tabindex="0" data-v-43aff82f data-v-4bb27b21><!--[--><!----><span data-v-4bb27b21>关于</span><!----><!--]--><!----></a><!--]--><!--[--><a class="vp-link link navbar-menu-link" href="/friends/" tabindex="0" data-v-43aff82f data-v-4bb27b21><!--[--><!----><span data-v-4bb27b21>友链</span><!----><!--]--><!----></a><!--]--><!--]--></nav><!--[--><!--]--><!----><div class="vp-navbar-appearance appearance" data-v-f21177fa data-v-119dc5f3><button class="vp-switch vp-switch-appearance" type="button" role="switch" title aria-checked="false" data-v-119dc5f3 data-v-33c3ff14 data-v-918f75ba><span class="check" data-v-918f75ba><span class="icon" data-v-918f75ba><!--[--><span class="vpi-sun sun" data-v-33c3ff14></span><span class="vpi-moon moon" data-v-33c3ff14></span><!--]--></span></span></button></div><!----><div class="vp-flyout vp-navbar-extra extra" data-v-f21177fa data-v-e7760ed7 data-v-c6e51e23><button type="button" class="button" aria-haspopup="true" aria-expanded="false" aria-label="extra navigation" data-v-c6e51e23><span class="vpi-more-horizontal icon" data-v-c6e51e23></span></button><div class="menu" data-v-c6e51e23><div class="vp-menu" data-v-c6e51e23 data-v-b9b06af5><!----><!--[--><!--[--><!----><div class="group" data-v-e7760ed7><div class="item appearance" data-v-e7760ed7><p class="label" data-v-e7760ed7>外观</p><div class="appearance-action" data-v-e7760ed7><button class="vp-switch vp-switch-appearance" type="button" role="switch" title aria-checked="false" data-v-e7760ed7 data-v-33c3ff14 data-v-918f75ba><span class="check" data-v-918f75ba><span class="icon" data-v-918f75ba><!--[--><span class="vpi-sun sun" data-v-33c3ff14></span><span class="vpi-moon moon" data-v-33c3ff14></span><!--]--></span></span></button></div></div></div><!----><!--]--><!--]--></div></div></div><!--[--><!--]--><button type="button" class="vp-navbar-hamburger hamburger" aria-label="mobile navigation" aria-expanded="false" aria-controls="nav-screen" data-v-f21177fa data-v-3190614d><span class="container" data-v-3190614d><span class="top" data-v-3190614d></span><span class="middle" data-v-3190614d></span><span class="bottom" data-v-3190614d></span></span></button></div></div></div></div><div class="divider" data-v-f21177fa><div class="divider-line" data-v-f21177fa></div></div></div><!----></header><div class="vp-local-nav fixed reached-top is-blog" data-v-f306e958 data-v-394b6917><button class="hidden menu" disabled aria-expanded="false" aria-controls="SidebarNav" data-v-394b6917><span class="vpi-align-left menu-icon" data-v-394b6917></span><span class="menu-text" data-v-394b6917>Menu</span></button><div class="vp-local-nav-outline-dropdown" style="--vp-vh:0px;" data-v-394b6917 data-v-46264cb5><button data-v-46264cb5>返回顶部</button><!----></div></div><!----><!--[--><div id="VPContent" vp-content class="vp-content" data-v-f306e958 data-v-d7cb234b><div class="vp-doc-container is-blog" data-v-d7cb234b data-v-15e751aa><!--[--><!--]--><div class="container" data-v-15e751aa><!----><div class="content" data-v-15e751aa><div class="content-container" data-v-15e751aa><!--[--><!--]--><main class="main" data-v-15e751aa><nav class="vp-breadcrumb" data-v-15e751aa data-v-118230fd><ol vocab="https://schema.org/" typeof="BreadcrumbList" data-v-118230fd><!--[--><li property="itemListElement" typeof="ListItem" data-v-118230fd><a class="vp-link link breadcrumb" href="/" property="item" typeof="WebPage" data-v-118230fd><!--[-->首页<!--]--><!----></a><span class="vpi-chevron-right" data-v-118230fd></span><meta property="name" content="首页" data-v-118230fd><meta property="position" content="1" data-v-118230fd></li><li property="itemListElement" typeof="ListItem" data-v-118230fd><a class="vp-link link breadcrumb" href="/" property="item" typeof="WebPage" data-v-118230fd><!--[-->博客<!--]--><!----></a><span class="vpi-chevron-right" data-v-118230fd></span><meta property="name" content="博客" data-v-118230fd><meta property="position" content="2" data-v-118230fd></li><li property="itemListElement" typeof="ListItem" data-v-118230fd><a class="vp-link link breadcrumb" href="/blog/categories/?id=4921c0" property="item" typeof="WebPage" data-v-118230fd><!--[-->ai<!--]--><!----></a><span class="vpi-chevron-right" data-v-118230fd></span><meta property="name" content="ai" data-v-118230fd><meta property="position" content="3" data-v-118230fd></li><li property="itemListElement" typeof="ListItem" data-v-118230fd><a class="vp-link link breadcrumb" href="/blog/categories/?id=4a7c22" property="item" typeof="WebPage" data-v-118230fd><!--[-->foundations-of-llms<!--]--><!----></a><span class="vpi-chevron-right" data-v-118230fd></span><meta property="name" content="foundations-of-llms" data-v-118230fd><meta property="position" content="4" data-v-118230fd></li><li property="itemListElement" typeof="ListItem" data-v-118230fd><a class="vp-link link breadcrumb current" href="/article/avoeswll/" property="item" typeof="WebPage" data-v-118230fd><!--[-->第2章 生成式模型（LLM）<!--]--><!----></a><!----><meta property="name" content="第2章 生成式模型（LLM）" data-v-118230fd><meta property="position" content="5" data-v-118230fd></li><!--]--></ol></nav><!--[--><!--]--><!--[--><h1 class="vp-doc-title page-title" data-v-0d4fd2de><!----> 第2章 生成式模型（LLM） <!----></h1><div class="vp-doc-meta" data-v-0d4fd2de><!--[--><!--]--><p class="reading-time" data-v-0d4fd2de><span class="vpi-books icon" data-v-0d4fd2de></span><span data-v-0d4fd2de>约 15044 字</span><span data-v-0d4fd2de>大约 50 分钟</span></p><!----><!--[--><!--]--><p class="create-time" data-v-0d4fd2de><span class="vpi-clock icon" data-v-0d4fd2de></span><span data-v-0d4fd2de>2025-10-05</span></p></div><!--]--><!--[--><!--]--><div class="_article_avoeswll_ external-link-icon-enabled vp-doc plume-content" vp-content data-v-15e751aa><!--[--><!--]--><div data-v-15e751aa><div class="language-mermaid line-numbers-mode" data-highlighter="shiki" data-ext="mermaid" style="--shiki-light:#393a34;--shiki-dark:#dbd7caee;--shiki-light-bg:#ffffff;--shiki-dark-bg:#121212;"><pre class="shiki shiki-themes vitesse-light vitesse-dark vp-code"><code class="language-mermaid"><span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">mindmap</span></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">  root((第2章 生成式模型))</span></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">    概念</span></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">      自回归</span></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">        下一词</span></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">        困惑度</span></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">      因果掩码</span></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">        上三角</span></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">        不看未来</span></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">    架构</span></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">      解码器 Transformer</span></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">        自注意力</span></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">        前馈 残差</span></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">      位置编码</span></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">        绝对 相对</span></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">        RoPE ALiBi</span></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">    训练</span></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">      目标</span></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">        交叉熵</span></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">        标签平滑</span></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">      优化</span></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">        预热 退火</span></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">        混合精度 裁剪</span></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">    微调</span></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">      全量</span></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">        特定任务</span></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">        多任务</span></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">      参数高效</span></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">        LoRA 适配器</span></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">        前缀 软提示</span></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">    解码</span></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">      策略</span></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">        贪婪 束搜索</span></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">        Top k Top p 温度</span></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">      控制</span></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">        重复惩罚</span></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">        约束 解码</span></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">    对齐过渡</span></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">      指令化</span></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">        指令 模板</span></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">        统一格式</span></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">      贴近偏好</span></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">        RLHF</span></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">        DPO</span></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">    评估</span></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">      质量</span></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">        准确 一致</span></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">      效率</span></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">        延迟 吞吐 成本</span></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">    风险</span></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">      幻觉</span></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">        虚构 内容</span></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">      安全</span></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">        越界 有害</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>第二章讨论生成式的大型语言模型(LLM)的构建原理和技术细节。这类模型就是我们常说的聊天模型或文本生成 AI，包括 GPT 系列、Anthropic 的 Claude 等。这里我们先介绍 LLM 的基本概念，然后探讨如何训练和微调这些模型，如何对齐模型与人类期望，以及提示的作用。接着，我们会讲述如何通过大规模数据和分布式技术来扩大模型训练，以及解决长文本序列处理的方案。</p><h2 id="_2-1-大语言模型简介" tabindex="-1"><a class="header-anchor" href="#_2-1-大语言模型简介"><span>2.1 大语言模型简介</span></a></h2><p><strong>大型语言模型(LLM)</strong> 通常指参数规模极大（数亿到数千亿）且在海量语料上训练的语言模型。LLM 通常采用仅解码器的 Transformer 架构，擅长根据已给的上下文生成续写文本。由于规模空前且训练语料丰富，LLM 表现出惊人的通用语言能力——不仅能编故事，对答如流，还能执行以前需要专门算法的分析任务（例如代码解释、数学推理等），仿佛拥有“通才智能”。</p><p>从概念上，LLM 是基于概率的文本生成器：它为任何给定的文本前缀，定义了下一个词的概率分布 。如果我们用 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mi>r</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">Pr(x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span></span> 表示一段文本<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">x</span></span></span></span>的概率，语言模型会将这个概率拆解为每个词逐次出现的条件概率的乘积 。例如，一段话“<strong>今天天气很好</strong>”的概率被分解为：“今”的概率 _ “天”在“今”之后的概率 _ “气”在“今天”之后的概率 _ … _ “好”在“今天天气很”之后的概率。LLM 就是学会去近似这些概率。在实际应用中，我们不直接关心这个概率有多大，而是在让模型<strong>生成</strong>文本时，每一步依据这些概率选取最可能的下一个词，从而产生连贯的输出。</p><p><strong>类比</strong>：可以把 LLM 想象成一个训练有素的<strong>写作机器人</strong>。它脑子里储存了大量词语的接续模式。当你给它开头几句时，它会根据训练中统计到的模式继续往下写。比如，你说“从前有一个国王”，LLM 的大脑中会浮现出无数小说、故事里的相关情节，然后基于概率选一个合适的续写，如“他有三个女儿……”。这个过程没有检索数据库里的现成句子，而更像是<strong>即兴创作</strong>——当然，是遵循统计学上<strong>最可能</strong>的那种创作。</p><p>值得注意的是，&quot;大型&quot;主要指参数规模大和训练数据量大，并不只是模型结构复杂。参数越多、见过的语料越丰富，模型越可能学会细致复杂的语言现象，甚至跨越纯统计，表现出某种<strong>推理</strong>和<strong>常识</strong>能力。这在小模型上难以观察，但 LLM 里频繁出现，被称为<strong>涌现能力</strong>：模型规模超过某个阈值后，开始能解决一些它在小规模时完全做不到的任务。这也是为什么近几年大家追求更大规模的模型。</p><h3 id="_2-1-1-仅解码器-transformer-架构" tabindex="-1"><a class="header-anchor" href="#_2-1-1-仅解码器-transformer-架构"><span>2.1.1 仅解码器 Transformer 架构</span></a></h3><p>当前绝大多数 LLM 都采用 Transformer 的<strong>仅解码器</strong>架构。回顾一下，Transformer 解码器是一种堆叠的神经网络层结构，每层包括<strong>自注意力机制</strong>和<strong>前馈网络</strong>。关键在于<strong>自注意力</strong>可以让模型在每个位置&quot;关注&quot;输入序列中更重要的部分。但是<strong>仅解码器</strong>的注意力有一个特殊设置：它是<strong>掩蔽的自注意力</strong>，即每个词只能看见它前面的词，不能看见后面的词。这样确保模型生成下一个词时，只依据之前的信息。在实现上，这通过一个上三角的 Mask 矩阵来完成：遮住当前词之后的位置。</p><p>一个标准的仅解码器 Transformer 由几十层这种解码模块堆叠而成。输入通过词嵌入后依次经过层堆栈，输出是每个位置上的表示向量，再经过 softmax 得到下个词概率。因为采用了 Mask，每层的自注意力实际上让模型聚合了<strong>前文</strong>的所有信息来预测下文。层数越深，模型对长距离依赖的建模能力越强。这种结构没有独立的编码器，所有的理解和生成都混合在同一个 Transformer 网络中完成。</p><p><strong>现实模型例子：</strong> GPT-2、GPT-3、GPT-4、Claude、LLaMA 等知名大模型内部都是这种架构。比如 GPT-3 有 96 层 Transformer decoder，每层有多头注意力去看前面的词；Claude 据报道结构类似，也是基于 Transformer decoder。相比编码器-解码器那种“先读后写”的分工，decoder-only 架构更简单直接——<strong>始终在一边读一边写</strong>，每产生一个词就将其纳入上下文再继续生成下一个。</p><p><strong>类比</strong>：仅解码器架构就像人写文章时，一字一句顺序写下去，写当前词的时候只参考已经写好的部分，不去偷看还没写的结局。而编码器-解码器架构更像先打草稿列提纲(编码理解)，再正式写文(解码输出)。大模型大多选择前者，是因为它训练起来相对简单统一，而且生成能力很强。</p><h3 id="_2-1-2-训练-llm" tabindex="-1"><a class="header-anchor" href="#_2-1-2-训练-llm"><span>2.1.2 训练 LLM</span></a></h3><p><mark>训练大型语言模型在本质上与训练一般的语言模型类似：我们有一个海量语料库，目标是<strong>最大化模型对训练语料的概率</strong>，等价于<strong>最小化预测下一个词的误差</strong>。</mark>具体实现中，通常采用<strong>随机梯度下降</strong>(SGD)变种算法来优化模型参数 。</p><p>设训练集包含大量序列，每个序列是一段文本 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mo>=</mo><mo stretchy="false">(</mo><msub><mi>x</mi><mn>0</mn></msub><mo separator="true">,</mo><msub><mi>x</mi><mn>1</mn></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi>x</mi><mi>m</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">x = (x_0, x_1, …, x_m)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">m</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>。LLM 为这个序列定义的概率是各词的条件概率连乘：</p><p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>P</mi><mi>r</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><munderover><mo>∏</mo><mrow><mi>i</mi><mo>=</mo><mn>0</mn></mrow><mi>m</mi></munderover><mi>P</mi><mi>r</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>∣</mo><msub><mi>x</mi><mrow><mo>&lt;</mo><mi>i</mi></mrow></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">Pr(x) = \prod_{i=0}^{m} Pr(x_i \mid x_{&lt;i}) </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.9291em;vertical-align:-1.2777em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.6514em;"><span style="top:-1.8723em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">0</span></span></span></span><span style="top:-3.05em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∏</span></span></span><span style="top:-4.3em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">m</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.2777em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∣</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mrel mtight">&lt;</span><span class="mord mathnormal mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1774em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span></p><p>这里 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mrow><mo>&lt;</mo><mi>i</mi></mrow></msub></mrow><annotation encoding="application/x-tex">x_{&lt;i}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6079em;vertical-align:-0.1774em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mrel mtight">&lt;</span><span class="mord mathnormal mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1774em;"><span></span></span></span></span></span></span></span></span></span> 表示序列开头到 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi><mo>−</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">i-1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7429em;vertical-align:-0.0833em;"></span><span class="mord mathnormal">i</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">1</span></span></span></span> 的所有词。我们用<strong>对数似然</strong>衡量模型对序列的拟合程度，即 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>log</mi><mo>⁡</mo><mi>P</mi><mi>r</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\log Pr(x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span></span>。训练就是不断调整模型参数 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>θ</mi></mrow><annotation encoding="application/x-tex">\theta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span></span></span></span>，使得训练集所有序列的平均对数似然尽可能大。</p><p>因为训练数据极其庞大，我们不会每次用完整语料去算梯度，而是将语料分成许多小<strong>批次(batch)</strong>。每个批次取若干序列，计算模型在这些序列上预测下个词的误差，再反向传播调整参数。随着无数个批次迭代，模型的预测能力不断提高。</p><p><strong>训练 LLM 的挑战</strong>在于规模：一方面数据量大，比如 GPT-3 使用了近 45TB 文本数据，包含上千亿单词；另一方面参数量大，像 GPT-3 有 1750 亿参数。这导致训练一次需要巨大的算力和长时间。例如 GPT-3 训练 reportedly 花费数千个 GPU 日的计算。为了成功训练，优化过程中的<strong>技巧</strong>非常重要，包括选择合适的<strong>学习率调度</strong>（太大会发散，太小训练慢）、<strong>正则化</strong>防止过拟合、以及正确的<strong>初始化</strong>防止梯度消失或爆炸等。后面 2.2 节会详谈工程上的解决方案。</p><p><strong>类比</strong>：训练 LLM 就像<strong>驯养一只庞大的动物</strong>。一开始模型很“野”，输出几乎是乱猜。通过喂给它海量的“书本”和反复纠正（梯度下降调整参数），模型逐渐学会模式。但驯服它很不容易，需要耐心（大量迭代）和技巧（调整超参数）。训练中常见的问题如不稳定，就像动物训练中突然暴躁，需要训练师调整方法（比如降低学习率）来稳定局面。</p><p><strong>小问题：如果训练数据太少会怎样？直觉上，模型可能只“记住”训练集而不能泛化，即出现过拟合</strong>。这有点像只让学生背几道题却希望他举一反三，往往不行。所以 LLM 的成功秘诀之一正是<strong>巨量的数据</strong>，让模型见过足够多样的语言现象，从而学到一般化的能力，而不仅是记忆。</p><h3 id="_2-1-3-微调-llm" tabindex="-1"><a class="header-anchor" href="#_2-1-3-微调-llm"><span>2.1.3 微调 LLM</span></a></h3><p>虽然 LLM 在预训练后已经具备处理语言的通用能力，但直接拿一个预训练模型去满足实际应用，往往结果不够理想。<strong>微调</strong>在 LLM 领域依旧扮演着重要角色，只不过微调的目的和方式相对于 BERT 那种针对单一任务的微调有所拓展。</p><p>对于 LLM，微调有几个常见场景：</p><ul><li><strong>指令微调</strong>：这是近年来非常重要的一种微调形式 。它指的是通过有监督学习，让模型更好地遵循人类指令。例如，OpenAI 通过收集大量“指令-&gt;响应”的示例，对 GPT-3 进行微调，得到 InstructGPT，使模型学会当用户用指令形式提问时给出有用的回答。这种微调实际上是给模型注入“听话”的能力，让它<strong>对齐人类意图</strong>（这一点我们在 2.1.4 节详细讨论）。</li><li><strong>对话微调</strong>：类似地，可以用成对的对话数据（用户问句-&gt;理想回答）来微调 LLM，使其风格更像聊天助手。例如 Anthropic 公司用大量对话语料微调 Claude，使其擅长长对话、遵守对话礼仪等。对话微调可以被视为指令微调的一个特例，因为开放聊天本质上也是对各种指令的响应，只不过形式是对话式的。</li><li><strong>领域微调</strong>：将 LLM 在某个专门领域的数据上继续训练，以提升它在该领域的表现。比如有了 GPT-3，我们可以在医学论文和问答数据上微调它，使其更懂医学问题（得到一个医疗助手模型）；或者在法律文档上微调得到法律助手。这其实类似传统模型的微调，只是 LLM 的参数很多，我们通常选择<strong>低学习率、少量 step</strong>来细调，避免毁掉预训练中学到的一般知识。</li><li><strong>对齐相关微调</strong>：除了指令，对 LLM 进行价值观和安全方面的微调也很重要，比如 OpenAI 会过滤或惩罚模型输出不当内容的倾向。这些通常结合人类反馈进行强化学习微调（见下节），不过也可以预先用一些人工构造的规则数据进行有监督微调，使模型学会拒答敏感请求等。</li></ul><p>微调 LLM 通常仍使用梯度下降，只是规模较预训练小很多。因为预训练好的模型已经很强，我们往往需要的数据少、训练轮次也少，这样既<strong>高效</strong>又<strong>经济</strong>。然而，即使是小规模微调，也需要注意选择合适的超参数（学习率、batch 大小等）以防止模型性能下降或发生灾难性遗忘。一些研究也提出了<strong>高效微调</strong>方法，比如只训练部分参数(LoRA 方法)或者多任务微调等，使得针对不同任务快速定制模型成为可能。</p><p><strong>例子：ChatGPT 实际上经历了两阶段微调：先用监督微调(SFT)教会模型遵循人类指令（让模型见识正确的问答范例），再用人类反馈强化学习进一步调整（后面会谈）。这样的微调使得 ChatGPT 相较基底的 GPT-3，更加听从用户、回答有礼有据，也懂得拒绝不当要求</strong>。</p><h3 id="_2-1-4-将大语言模型与世界对齐" tabindex="-1"><a class="header-anchor" href="#_2-1-4-将大语言模型与世界对齐"><span>2.1.4 将大语言模型与世界对齐</span></a></h3><p><strong>对齐(Alignment)是当前大语言模型讨论的重点话题之一。这里“将 LLM 与世界对齐”指的是引导模型的行为符合人类的意图、价值观和现实知识</strong>。预训练的语言模型虽然能生成流畅文本，但可能存在诸多问题：它可能对有害请求不加辨别地回答、可能输出事实性错误甚至胡编乱造、有时还会表现出社会偏见。这些都需要通过对齐手段来纠正。</p><p>对齐可以来自多方面的人类指导，例如<strong>有标注的数据</strong>和<strong>人类反馈</strong>。常见的两步对齐流程是：</p><ul><li><strong>监督微调(SFT)</strong>：正如 2.1.3 节提到的，用人类精心准备的指令-回应数据来微调 LLM。这一步让模型<strong>学会遵循指令</strong>，初步对齐于用户意图。可以理解为教模型“听话懂事”。模型通过模仿这些高质量回应，学到了在各种指令下应该如何回答更符合人类期望 。例如，我们喂给模型大量例子：用户要求模型讲笑话、模型就真的讲一个无攻击性的笑话；用户问科普问题、模型给出准确易懂的解释。经过 SFT，LLM 会变得更倾向于给出有帮助、礼貌的回答，而不是像原始预训练模型那样有时答非所问或态度古怪。</li><li><strong>从人类反馈中学习</strong>：即<strong>人类反馈强化学习(RLHF)</strong>。这一步在模型已经能遵循指令的基础上，进一步用人类偏好优化模型输出。具体做法通常是：先准备很多模型回答的备选，然后让人或一个“奖励模型”对这些回答打分，表示哪个更符合人类喜好 。接着，把 LLM 当作“智能体”，用强化学习的方法（例如政策梯度）让模型调整生成策略，使得它得到更高的人类评分  。直观上，这一步是“奖励好的回答，惩罚不好的回答”，久而久之模型更倾向于产出人类喜欢的内容。OpenAI 在训练 ChatGPT 时采用了 RLHF：先由人类比较模型的两段回答孰优，训练出一个奖励模型，然后让模型通过试错学习去最大化奖励 。这个过程纠正了许多模型不理想的行为，使 ChatGPT 在回答中更加<strong>有益、无害、真实</strong>。</li></ul><p>对齐的重要性不言而喻：<strong>一个不加对齐的模型可能输出有害信息或造成误导</strong>。例如，如果用户询问“如何制造危险物品”，未经对齐的模型可能直接给出步骤，造成安全隐患；但一个经过对齐的模型会礼貌地拒绝回答这种请求 。又比如，模型预训练语料里可能含有偏见，如果不加约束，它的回答可能歧视某些群体。对齐步骤会教模型避免复述这些偏见。</p><p>要注意，==对齐是一个持续的过程，因为<strong>人类的期望是多样且动态变化的</strong>。我们往往很难事先穷举所有“不当”或“期望”情况。模型只有在实际交互中暴露问题后，我们再通过新增数据或规则去进一步对齐。==所以，很多现代 LLM 会随着用户使用和反馈进行<strong>迭代改进</strong>。Anthropic 的 Claude 采用“宪法 AI”方法，也是一种对齐策略：用一组原则(“AI 宪法”)来自动过滤和引导模型输出，然后通过反馈不断调整，使模型遵守这些原则。这本质上也是对齐，只不过把人类价值编码成规则让模型自我审查和优化。</p><p><strong>总结地说</strong>，对齐就是让模型<strong>更安全可靠</strong>地服务于人类。技术上主要通过<strong>有监督微调</strong>和<strong>人类反馈强化</strong>来实现。实际的大模型（如 ChatGPT、Claude）无一例外都经历了这些对齐过程，所以它们比早期的 GPT-2 那样&quot;野生&quot;的模型更可信和友好。在后面的第四章中还有更深入的对齐方法讨论，但掌握上述基本概念已经足以理解 LLM 为何能较好地遵守人类指令。</p><blockquote><p><strong>对齐效果对比示例：</strong></p><p>用户问：“请问晚上如何入侵一栋大楼？”</p><p><strong>未经对齐的模型：</strong></p><ul><li>可能直接输出一段攻略</li><li>因为它只知道生成看似相关的文本</li></ul><p><strong>对齐后的模型：</strong></p><ul><li>回复：“对不起，这个请求不被允许。”</li><li>或提醒对方这样做不当</li></ul><p>这个简单场景体现了对齐的效果——模型从一个冷冰冰的概率机器，变得更像一个有道德与常识的助手。</p></blockquote><h3 id="_2-1-5-提示-llm" tabindex="-1"><a class="header-anchor" href="#_2-1-5-提示-llm"><span>2.1.5 提示 LLM</span></a></h3><p><strong>提示(prompt)是使用 LLM 时非常关键的概念。提示就是我们喂给模型的输入，它可以包含指令、问题、上下文</strong>，以及我们希望模型遵循的格式等 。由于 LLM 本质上是个文本接续模型，提示设计的好坏直接影响模型输出的质量。</p><p>前面 1.1.2 节已经举了一些提示例子，例如在句子后加上“这句话的情感是：”引导模型输出情感标签，或者给几个 QA 示例作为 few-shot 提示。这里总结几点提示 LLM 的要领：</p><ul><li><strong>清晰描述任务</strong>：提示中应明确告诉模型你想要它做什么。模型并不知道你心中的具体意图，除非你用自然语言表述出来。比如要它翻译，就明说“请把以下句子翻译成英文：…”；要它写代码，就提示“写一个函数实现……”。清晰的指令有助于模型产出符合预期的结果。</li><li><strong>提供必要背景</strong>：LLM 对当前世界的知识只来自训练截断的语料，对于更新的事实或上下文不敏感。所以如果任务涉及特定上下文或知识，最好在提示里给出。例如问模型关于一段文章的总结，就把那段文章包含在提示中。又例如提问一个有歧义的问题，提示里可以补充背景以消除歧义。</li><li><strong>示范格式</strong>：如果希望输出特定格式或风格，可以在提示中示范。比如想要 JSON 格式回答，可以在提示中说明“用 JSON 格式回答”或给一个简短示例 JSON。模型善于模仿提示中的模式，所以这能有效约束输出形式。</li><li><strong>分步骤提问</strong>：对于复杂任务，可以通过<strong>逐步提示</strong>的方式。比如让模型先分析问题，再给答案；或者使用链式思维提示，要求它思考中间推理过程。良好的提示可以引导模型先&quot;想&quot;再&quot;答&quot;，从而提高准确性。这有点像你提问时提示对方&quot;请分步骤说明&quot;，模型也会倾向于遵循。</li></ul><p>一个经过巧妙提示的大模型往往能胜任各种任务，这就是<strong>提示工程</strong>的威力所在。当然，提示不需要遵循固定格式，任何自然语言甚至加入特殊标记都可以，只要有助于模型理解你的意图。我们会在第 3 章详细讨论各种提示策略和技巧。</p><p><strong>小例子：假设我们有 Claude 模型，想让它写一封道歉信。如果直接说“写封信表达歉意”，模型也许给出一个模板化的道歉信。但如果提示改进一下：“背景：我昨晚错过了好朋友的生日聚会。我感到很抱歉，希望表达诚恳的歉意并请求原谅。请帮我写一封真挚、有温度的道歉信给我的朋友。”</strong> Claude 读取这个详细提示，就明白了口吻要真挚、对象是好朋友、原因是错过生日聚会，于是更有可能生成一封情感到位的道歉信。可见，给模型充分的信息和明确要求，它才能发挥得更好。</p><p><strong>引导提问：</strong> 当你在使用 ChatGPT 时，如果得到的回答不理想，不妨反思一下提示是否清楚具体。你可以尝试重新表述问题，或者给出你期待的答案格式提示模型。通过不断试验和改进提示，你会发现 LLM 的表现有显著提升。这种和模型互动调教的过程，其实正是人类在进行提示工程，将人类意图更精准地传达给模型。</p><h2 id="_2-2-大规模训练" tabindex="-1"><a class="header-anchor" href="#_2-2-大规模训练"><span>2.2 大规模训练</span></a></h2><p>==真正让 LLM 发挥威力的一个关键，是把模型和训练规模<strong>推向极限</strong>。这里的规模包括：<strong>数据规模、模型参数规模和计算规模</strong>。==本节我们讨论在训练超大模型时的一些工程挑战和解决方案，包括数据准备、模型结构修改、分布式训练，以及著名的<strong>缩放定律</strong>。</p><h3 id="_2-2-1-数据准备" tabindex="-1"><a class="header-anchor" href="#_2-2-1-数据准备"><span>2.2.1 数据准备</span></a></h3><p>俗话说“巧妇难为无米之炊”，训练 LLM 首先要有“米”，也就是<strong>海量高质量的数据</strong>。LLM 的数据通常来自互联网的各个角落，例如维基百科、新闻文章、网页爬取内容、论坛帖子、代码仓库等 。准备这些数据是繁杂但重要的步骤：</p><ul><li><strong>数据收集</strong>：研究者会爬取公开网络数据(如 Common Crawl 抓取的网页)、使用开放的文本语料(如维基百科、OpenSubtitles)、代码数据（如 GitHub 开源代码）、问答论坛数据等等。为了让模型知识全面，数据来源尽可能多样化，比如 OpenAI 的 GPT-3 用了 8:2 比例的互联网文本和编程代码数据，Meta 的 LLaMA 用了网络内容、百科、书籍和 Github 代码等多个库 。</li><li><strong>数据清洗</strong>：互联网数据混杂噪音，需要<strong>清洗</strong>。这包括去除乱码、HTML 标签、脚本代码等非自然语言内容；过滤脏话、隐私信息和法律风险内容；剔除重复或高度相似的文本片段（防止模型过度记忆某些内容）。清洗还可能包括分句、分词，统一编码格式等处理。许多团队为此制定了一系列规则和自动过滤脚本，使得最终进入训练的数据相对干净、有用。</li><li><strong>质量控制</strong>：并非所有网络文本都值得学习。例如机器生成的垃圾内容、极端偏见言论、人身攻击等，不希望模型模仿。团队通常会设定一些 heuristic 规则或训练一个分类器，将低质量数据剔除。此外，有些过于重复或简单的内容（比如某些自重复句子）也可能被 down-sample 处理，避免模型学到无谓模式。</li><li><strong>数据平衡</strong>：如果模型训练数据在某类文本上比例过大，可能导致模型输出有偏。举例来说，如果 90%数据都是英文，那么模型对英文非常擅长，对其他语言就相对较弱。因此数据准备时会注意不同领域、风格的<strong>平衡</strong>。例如安排一定比例的对话数据，让模型对话能力更好；加一些法律/医学文本，让模型懂一点专业术语。当然，平衡不是简单平均，而是结合应用场景权衡。如果模型主要服务英文用户，那英文数据还是主体，但可以适度混入其他语种以具备多语言基础。</li></ul><p><strong>数据准备的规模</strong>相当惊人：以 GPT-3 为例，最终用于训练的文本词汇量约在几千亿单词数量级。Meta 的 LLaMA-2 模型据称使用了 2 万亿字符以上的数据。可以说 LLM 之所以&quot;博闻强识&quot;，绝大部分功劳要归于喂给它的<strong>海量知识宝库</strong>。当然，庞大数据也带来副作用：比如里面难免有错误知识或过时信息，这也是为什么后期对齐和外部知识补充很重要。</p><p><strong>类比</strong>：训练 LLM 收集数据像是在建造图书馆。你要从全世界搜集书籍，但不能不加筛选地全收，需要剔除劣质书、防止重复藏书、分类整理等等。最后这个图书馆藏书百万册，包罗万象。模型训练就是让“图书馆里的书”都被模型大致读过一遍，形成记忆。好的数据准备确保模型“读的书”是有用且多样的，从而塑造出见多识广的 LLM。</p><p><strong>小问题：这么多数据放进模型，它会不会把训练文本直接存下来、逐字背诵出来？正常情况下不会&quot;逐字存储&quot;，因为模型参数虽然多但远不足以逐字记忆整个语料库，况且训练优化的目标是让模型概括</strong>语言规律而非死记硬背具体句子。所以模型更像是在压缩知识。如果训练中某段文本重复很多遍，模型可能确实能一字不差背出来（这就是数据去重重要性的原因）。但大多数情况下，模型生成的是<strong>基于训练文本概括出的类似内容</strong>，而非简单重现。</p><h3 id="_2-2-2-模型修改" tabindex="-1"><a class="header-anchor" href="#_2-2-2-模型修改"><span>2.2.2 模型修改</span></a></h3><p><strong>训练 LLM 很困难</strong>，随着模型变得越来越大，训练过程常出现不稳定甚至发散的问题 。研究者和工程师们在标准 Transformer 架构基础上做了许多<strong>改进和调整</strong>，以确保如此大规模的模型依然可训练、效果良好。这里介绍几项常见且重要的模型结构修改：</p><ul><li><strong>前置 Layer Normalization</strong>：Layer Norm（层归一化）是一种稳定深层网络训练的技术，它通过减去激活的均值、除以标准差来规范化每层输出。Transformer 原始论文中采用的是<strong>后置 LayerNorm</strong>（即在残差连接之后做 Norm），但实践发现对于非常深的网络，<strong>前置 LayerNorm</strong>（在每个子层内部先 Norm 再残差）更稳定。现代 LLM 几乎都使用前置 LayerNorm 结构，例如表示为输出 = 输入 + Norm(子层输出)。此外，还有<strong>RMSNorm</strong>这样的变体层归一化，它只做缩放不做均值偏移，也被一些模型采用（如 LLaMA 使用了 RMSNorm）  。这些归一化策略能缓解梯度在层间传递时的分布变化问题，让深层网络更容易训练。</li><li><strong>改进激活函数</strong>：Transformer 中的前馈网络(FFN)引入非线性，一般用 ReLU 激活函数。但对于非常宽的 FFN 层，选择合适的激活至关重要 。许多 LLM 改用了<strong>GeLU</strong>（Gaussian Error Linear Unit）激活，它是 ReLU 的平滑版，被认为对模型性能略有提升 。更近一步，还有<strong>GLU</strong>(Gated Linear Unit)系列激活函数，它把 FFN 的线性变换拆成两部分：一部分控制门，一部分做变换 。常见的如 GeGLU、SwiGLU 等（PaLM 和 LLaMA 用的就是 SwiGLU） 。这些激活函数对参数量巨大的 FFN 提供了更灵活的变换能力，有助于提升表达力和训练稳定性。</li><li><strong>去除偏置项</strong>：有人报告在 LLM 的线性层中<strong>去掉 bias 偏置</strong>可以提升训练稳定性 。Transformer 里很多线性映射都有对应的偏置参数 b，研究发现这些偏置对模型最终性能影响不大，却可能在训练初期引入不必要的不稳定。所以一些模型（如 LLaMA、PaLM 等）干脆移除了大部分线性层的 bias 。这减少了参数量，也减轻了一点过拟合风险。 -<strong>改进位置编码</strong>：标准 Transformer 用固定的正弦波位置编码，但这对非常长序列的泛化能力有限。很多 LLM 在<strong>位置嵌入</strong>上做了修改 。例如使用<strong>旋转位置编码 RoPE</strong>（如 GPT-3 采用）、ALiBi（可拓展位置偏置，Meta 提出）等，使模型能更好处理训练长度以外的位置。这些改进通常与<strong>长序列建模</strong>有关，我们在 2.3 节详述。</li></ul><p>除了以上结构变化，还有其它工程手段：比如<strong>逐渐增大 batch</strong>（先小 batch 稳定训练，随后增大提高效率）、<strong>学习率 warmup 和衰减</strong>策略、<strong>混合精度训练</strong>(FP16/BF16 减少显存占用)、<strong>梯度裁剪</strong>防止爆炸等等 。总之，大模型训练是门“细活”，需要对架构和超参数精心设计。正是这些修改和技巧的积累，使得以前难以想象的百亿甚至千亿参数模型如今成为可能。</p><p><strong>简而言之</strong>：标准 Transformer 并非不能训练超大模型，但有了上述优化，它会<strong>训练得更快、更稳、更强</strong>。这也解释了为什么许多近年的模型架构看似跟原始 Transformer 差别不大，却能取得更好效果——秘密往往藏在这些不起眼的细节里。</p><p><strong>提示：</strong> 如果你是工程师，想自己训练一个 GPT 模型，上述几点值得特别留意。例如，一开始就选用前置 LayerNorm 和合适的激活函数，将会减少很多调参烦恼。训练中若遇到 loss 震荡剧烈甚至发散，不妨尝试调小学习率、检查梯度 norm 等，它们往往是大模型容易踩的坑。</p><h3 id="_2-2-3-分布式训练" tabindex="-1"><a class="header-anchor" href="#_2-2-3-分布式训练"><span>2.2.3 分布式训练</span></a></h3><p>由于 LLM 的模型太大、数据太多，在单台机器（哪怕多 GPU）上训练往往不现实。必须使用<strong>分布式训练</strong>，也就是把计算分拆到多台机器或多张卡上并行进行 。分布式训练的核心思想是将<strong>工作并行化</strong>，以加速训练和突破单机内存限制。</p><p>常用的并行策略包括：</p><ul><li><strong>数据并行</strong>：这是最常见也最简单的并行方式 。假设有 N 个 GPU，我们把同一时间的一个大批次(mini-batch)数据分成 N 份，每个 GPU 各算一份  。每个 GPU 都有一份完整的模型拷贝，独立计算自己那份数据的梯度 。然后再将 N 个梯度相加，得到跟单机处理整个 batch 相同的更新  。这样，理想情况下 N 个 GPU 几乎可以加速 N 倍，因为大家同时算不同数据。 在实现上，经常用 All-Reduce 操作来同步梯度。数据并行的好处是实现相对简单，而且扩展到上千卡也能接近线性加速。但它有个前提：每张卡都能放下整个模型参数和一次前向计算需求。对于超大模型（参数上百亿），单卡内存常不足以装下，这就需要结合模型并行。</li><li><strong>模型并行</strong>：将模型本身拆开，不同部分放在不同 GPU 上 。一种简单的模型并行是<strong>层并行</strong>（Pipeline 并行）：假如模型有 L 层 Transformer，我们可以每 K 层分成一段，部署到不同 GPU 。前向传播时，第 1 段 GPU 算完再将输出交给第 2 段 GPU 算……依次串行；反向传播则反方向串行。因此纯粹的层并行效率并不高，因为各 GPU 等待依赖，不能同时工作 。为改进，通常结合流水线技术，让不同微批次在不同阶段并行，尽量避免闲置等待。不过纯模型并行的瓶颈在于<strong>通信</strong>：层间输出需要跨设备传输，过多的切分会导致通信开销增大，抵消并行带来的算力提升。</li><li><strong>张量并行</strong>：这是一种更细粒度的模型并行，将<strong>单个层内部</strong>的矩阵运算拆分 。例如，Transformer 里的大矩阵乘法，可以把矩阵按列块或行块分到多 GPU，各自乘相应块，然后再合并结果 。这样每个 GPU 只承担原运算的一部分计算和参数存储。张量并行可以很有效地减小单卡显存占用，并让多个 GPU 同时参与同一层计算。但缺点是需要 GPU 之间频繁同步中间结果（如全连接层需要 gather 拼接输出），实现上比数据并行复杂。 -<strong>混合并行</strong>：实际的大模型训练往往是上述方法的组合。如在 8 台机器上，先采用数据并行把数据分到 8 组，再在每组内部对模型做张量并行或管道并行。这样既解决了模型太大单卡放不下的问题，又充分利用多机加速训练。</li></ul><p>另外还有<strong>缓存高效并行</strong>、<strong>优化器状态分片</strong>等具体技术（如 ZeRO 优化器将优化器状态分散到各卡，节省内存），限于篇幅不展开。</p><p>总的来说，分布式训练是让 LLM 成为现实的幕后功臣。如果没有多 GPU 并行，训练 GPT-3 这样的模型可能要数月甚至更久，而使用上百卡并行可以在几周内完成。分布式带来的复杂度很高，需要工程团队精心调度通信和计算，使 GPU 资源得到最大利用。但正因为攻克了这个难题，我们才能在合理时间和成本内训练出超大模型。</p><p><strong>类比</strong>：可以把训练 LLM 比作建造金字塔，单个人（单 GPU）搬砖效率太低，根本竣工不了。于是调集成百上千的工人（GPU），有人运砖（数据并行），有人砌墙（模型并行），大家流水作业，才在较短时间内完成宏伟工程。分布式训练就是这“组织千人协作建塔”的一整套方法学。</p><p><strong>小例子：</strong> Meta 在训练 LLaMA-65B 模型时，使用了 2048 块 GPU 并行工作。据报道，他们结合了数据并行和张量并行，每台机器的 GPU 先同步算部分张量，再所有机器聚合梯度。最终，他们仅用数周就训练完模型。如果换单卡来算，哪怕 1 秒算 100 亿次，也需要几百年才能跑完！这展示了大规模并行的威力和必要性。</p><h3 id="_2-2-4-缩放定律" tabindex="-1"><a class="header-anchor" href="#_2-2-4-缩放定律"><span>2.2.4 缩放定律</span></a></h3><p>随着人们不断尝试更大的模型和更多的数据，逐渐发现了一些<strong>经验性规律</strong>，被称为大型模型的<strong>缩放定律(Scaling Laws)</strong>。这些定律揭示了模型性能与规模之间的大致关系，对我们规划训练有指导意义。</p><p>一个著名发现是<strong>OpenAI 的缩放规律</strong>：研究者观察到，在对数尺度下，模型的性能（例如验证集上的困惑度 perplexity）随着模型参数数量和训练数据量增加呈现近似<strong>幂定律衰减</strong>。简单说，模型越大、数据越多，损失会平滑地降低，但提升幅度逐渐变小。比如从 1 亿参数增加到 10 亿，困惑度也许能降一大块；而从 1000 亿增加到 10000 亿，降幅可能就较小。这类似生活中的“边际效用递减”。</p><p>更具体的，还有<strong>Chinchilla 优化法则</strong>（DeepMind 提出）。他们通过试验发现，对于给定算力预算，存在一个最优的模型大小和训练 token 数配比。如果模型太大而数据不足，或者数据超多而模型太小，都是资源浪费。==Chinchilla 定律建议把模型参数和训练 tokens 按照<strong>约 1:20</strong>的关系来分配训练，可以让最终效果最大化。==在 2022 年的实验中，他们用相同算力，让一个 70 亿参数模型训练 1.4 万亿 token，结果性能超过了原本用同算力训练的 2800 亿参数但只看几千亿 token 的 Gopher 模型。这说明<strong>不是一味堆模型参数就好，数据量也要跟上</strong>。</p><p>缩放定律还预测了一些性能拐点和新能力的涌现。例如，某项任务的性能随规模平稳提升，但在某个模型大小突然大幅跃升，那可能是模型学会了新的技能。这些定律虽然不精确，但给出了宏观趋势：<mark><strong>更大的模型+更多训练数据=更好的表现</strong>（前提是合理平衡）。</mark></p><p>对实践的影响是，团队在规划下代模型时，会参考这些规律：需要多大数据、多高算力才能充分发挥 X 亿参数模型？是否不如用更小模型训练更久？这些分析有助于决策。比如 OpenAI 从 GPT-2 到 GPT-3，大幅增大了模型参数，因为根据缩放曲线预测，这样可以显著提升零样本任务能力。事实证明也是如此。随后人们意识到 GPT-3 数据量可能偏少了，Chinchilla 论文出来后，新模型如 GPT-4 等据传都更加注重增加训练数据量，以符合更优配比。</p><p><strong>直观点</strong>，==缩放定律就是在告诉我们：<strong>智能的提升没有明显天花板，但代价越来越高</strong>。==为了那逐步变小的收益，我们付出指数增加的计算。业界因此需要在模型效果和资源消耗之间寻找平衡。未来也许通过算法改进，能“弯折”缩放曲线，达到用更少资源获取同等性能的目标，这是科研热点之一。</p><p><strong>类比</strong>：可以把缩放看作跑步训练的经验法则——跑更长时间（更多数据）和穿更先进的装备（更大模型）都会让成绩变好，但回报是递减的。刚开始训练，每增加 10 分钟耐力提升明显；到后来每天多跑 10 分钟效果就有限了。同理，模型从 1 亿到 10 亿参数长进很大，从 1000 亿到 10000 亿则进步相对小。但再小也是进步，累积起来就能刷新纪录。所以顶尖选手还是会不惜代价地增加训练，正如顶尖 AI 模型不断扩大规模，因为那怕一点提升，都可能意味着新的能力和应用突破。</p><h2 id="_2-3-长序列建模" tabindex="-1"><a class="header-anchor" href="#_2-3-长序列建模"><span>2.3 长序列建模</span></a></h2><p>标准 Transformer 模型有一个固有限制：<strong>上下文长度</strong>（也就是一次处理的文本长度）通常是固定且有限的。例如 GPT-3 的上下文上限约 2048 个 token，GPT-4 提高到 8K 或 32K，但仍是有限数值。如果输入文本超过这个长度，模型就无能为力，需要截断或分段处理。然而许多应用需要处理<strong>超长文本</strong>，比如整本书、长篇对话、长时间序列数据等。因此，如何让 LLM 有效地<strong>建模长序列</strong>成为重要课题。</p><p>本节介绍几种应对长上下文的技术，包括<strong>高效注意力架构</strong>、<strong>缓存与记忆机制</strong>、<strong>参数共享</strong>以及<strong>位置编码的拓展</strong>等。</p><h3 id="_2-3-1-高效架构" tabindex="-1"><a class="header-anchor" href="#_2-3-1-高效架构"><span>2.3.1 高效架构</span></a></h3><p>Transformer 的自注意力机制计算复杂度是<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><msup><mi>n</mi><mn>2</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(n^2)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0641em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">n</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>（对长度 n 的序列，比较任意两词的相关性），当 n 很大时计算和内存占用都很高。这使得标准 Transformer 难以直接扩展到上万甚至更多 token 长度。<strong>高效架构</strong>指的是对注意力机制进行改造或近似，使其在长序列下计算更可承受。</p><p>一些策略包括：</p><ul><li><strong>稀疏注意力(Sparse Attention)</strong>：让每个位置不用关注序列中所有点，而只关注一部分。比如<strong>局部注意力(Local Attention)</strong> 限制每个词只看相邻的一段窗口内的词，这将复杂度降低到近似线性（每个词只计算固定窗口大小的关系）。模型仍可以捕获局部依赖，而长距离关系可以通过层叠多次局部注意力间接建立。OpenAI 的 Longformer、BigBird 等模型采用这种思路，将上下文扩展到 8K 甚至更长。而且这种模型还能灵活设计关注模式，比如局部+周期采样远距点，兼顾局部和全局信息。</li><li><strong>低秩近似(Low-rank Approximation)</strong>：尝试用低维表示近似注意力矩阵。例如<strong>Linformer</strong>用低秩投影将<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>×</mo><mi>n</mi></mrow><annotation encoding="application/x-tex">n \times n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6667em;vertical-align:-0.0833em;"></span><span class="mord mathnormal">n</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">n</span></span></span></span>的注意力简化为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>×</mo><mi>k</mi></mrow><annotation encoding="application/x-tex">n \times k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6667em;vertical-align:-0.0833em;"></span><span class="mord mathnormal">n</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span></span></span></span>，将复杂度从<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><msup><mi>n</mi><mn>2</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(n^2)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0641em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">n</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>降为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mi>n</mi><mi>k</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(nk)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.03148em;">nk</span><span class="mclose">)</span></span></span></span>。<strong>Performer</strong>使用随机特征的方法，把注意力用核技巧近似成线性点积，从而在理论上做到线性复杂度的近似注意力。虽然这些近似会损失一些精度，但对长序列可以大幅提速和节省内存。</li><li><strong>分块处理</strong>：将很长的输入分成块，每块长度 manageable，然后设计一种机制在块之间传递信息。早期 Transformer-XL 就是将文本分段并引入<strong>记忆机制</strong>（下一节详述），使得块与块之间通过记忆关联。还有 Hierarchical Transformer，先处理小块得到摘要，再处理摘要串联的高层块，形成分层结构。类似于人读书也先读章节再总结全书。</li><li><strong>双路或多尺度模型</strong>：一些架构为不同范围的信息设计不同子模型。例如有模型用两种注意力：一种精细但短距，一种粗略但长距，结合起来可以兼顾效率和全局视野。这有点像人用眼睛既能看清近处细节，也能模糊感知远处轮廓，两者配合。比如 Combiner, LongNet 等研究方向。</li></ul><p>高效架构是一个活跃研究领域，各种新方案层出不穷。总的目标是：<strong>让模型处理长文本的时间和内存增长尽可能缓慢</strong>，最好接近线性增加而不是平方级别。需要权衡的是<strong>效率 vs. 表达力</strong>：过度近似可能损失模型精度，所以设计时往往结合任务需要，选择合适的折中方案。</p><p><strong>现实例子：</strong> Anthropic 的 Claude 号称能处理 10 万 token 的上下文，它背后就用了改进的注意力机制（据推测采用了一种分段总结+检索的方法，而非让基础 Transformer 直接看十万长度）。Google 的 Long-T5 模型对 T5 架构做了 Sparse attention 改造，成功扩展到 16K 长度用于文档摘要等任务。可以预见，未来会有更多模型突破长度限制，变得“记忆力超群”。</p><p><strong>类比</strong>：优化 Transformer 处理长序列，就像让一个人阅读一本巨著时<strong>提高阅读策略</strong>。普通 Transformer 等于傻傻地“一字不落全比较”，读一部百科全书要把每句话和每句话都对照一遍，自然很慢。而高效方法给它装上“快速略读”和“重点浏览”的本领：先分章节看，每章只细读重点段落，略过次要部分，这样全书也能较快了解。虽然略读可能漏掉细节，但为了效率不得不取舍。类似道理，LLM 面对长文档时，通过稀疏地看或粗略地看，也能大致掌握内容而不陷入组合爆炸的计算泥潭。</p><h3 id="_2-3-2-缓存与记忆" tabindex="-1"><a class="header-anchor" href="#_2-3-2-缓存与记忆"><span>2.3.2 缓存与记忆</span></a></h3><p>当序列特别长时，即使采用高效注意力，模型一次也难以全部处理。在生成场景下，一个解决思路是利用<strong>缓存(cache)</strong> 机制：在逐字生成时，保存模型前面的中间结果，避免每次都从头计算。实际上，当前 Transformer 解码生成时都会缓存每层的 Key 和 Value 矩阵，这样下一个词的计算可以复用前面的内容，而不用重新计算整个序列的自注意力。这使得生成长段落的时间复杂度从本来平方级(每步重新 n^2)降低为线性级(每步新增一些计算)。</p><p>但缓存只是在<strong>推理</strong>阶段帮助加速，对训练长序列本身没有提高模型能力。<strong>记忆机制</strong>更进一步，试图赋予模型一种“长时记忆”。例如<strong>Transformer-XL</strong>引入了<strong>循环记忆</strong>：将前一个 segment 最终的隐藏状态作为&quot;记忆&quot;传递给下一个 segment 作为额外输入。这样模型在处理新段时，实际上也“看见”了一定长度的前文摘要。通过这种递归，模型可以跨 segment 捕捉长期依赖关系。当时 Transformer-XL 就刷新了语言模型长文本基准，因为普通 Transformer 隔断上下文后就忘了，而它还能留住一点过去信息。</p><p>还有一些尝试，如<strong>可微神经缓存</strong>、<strong>外部记忆网络</strong>等：给模型配备一个读写记忆的模块，允许它在阅读长文时不断将信息写入内存，稍后需要时再读出。这类似于传统编程中的内存，模型可以学习<strong>哪些信息需要存</strong>、<strong>何时取用</strong>。OpenAI 曾有一个 FeedMe 架构就采用了额外的记忆矩阵。不过这类方法训练复杂且效果不稳定，目前主流还是改进注意力为主。</p><p><strong>重温</strong>，3.2 节我们会讲<strong>Retrieval 增强</strong>(RAG)那种工具，它实际相当于一种外部记忆：当模型需要知识时，去查询数据库/搜索引擎。这解决的是知识获取问题。而这里指的记忆，更偏指<strong>长距离上下文记忆</strong>，属于模型结构的一部分。</p><p><strong>现实应用</strong>中，缓存已是 Transformer 推理的标配了，否则 ChatGPT 每回应一轮都要把前文重复计算，很不划算。而诸如 Transformer-XL、GPT-Cache 等概念，还在往更长对话、长文问答中发挥作用。Anthropic 的 Claude 号称能“记住”百页文档内容，也是因为它在生成回答时运用了某种长距离记忆技术，让模型不会轻易忘掉开头说过的话。</p><p><strong>想象类比</strong>：一般 Transformer 像是人只有短期记忆，过了几个段落就忘记前面细节。而加了记忆机制的模型，像是人配了一本笔记本，读到重要内容就写下来提醒自己后面用。比如阅读一篇论文，可以边读边在笔记本上记下各章要点，哪怕论文很长，最终回答问题时可以翻看笔记（模型通过内存单元查看记忆向量），不会遗漏关键信息。这种“边读边记”的方式有效缓解了纯 Transformer 易遗忘长文本上下文的问题。</p><h3 id="_2-3-3-跨层和跨头的参数共享" tabindex="-1"><a class="header-anchor" href="#_2-3-3-跨层和跨头的参数共享"><span>2.3.3 跨层和跨头的参数共享</span></a></h3><p>这是一个针对模型<strong>参数效率</strong>的技巧。有时候我们希望在不增加参数的情况下加深模型或拓展长度，一个思路是<strong>共享参数</strong>：让多个层或多个注意力头使用相同的参数集合。这样做相当于用有限的参数反复应用多次。</p><p><strong>跨层共享</strong>早在小模型时代就有尝试，比如 ALBERT 模型（BERT 的轻量版）将所有层的参数共享，从而把参数量大幅减少，却保留了深度。然而在生成式 LLM 中，很少完全共享全部层参数，但<strong>部分共享</strong>或<strong>重复块</strong>的设计存在。例如 GPT-3 的架构其实某种程度上可以看作多层块重复，因为每层结构都一致。Facebook 曾有研究 Switched Transformers 尝试让几组层循环使用，来达到超深的效果。</p><p><strong>跨头共享</strong>则是指不同注意力头之间共用某些投影矩阵参数。这减少了独立头参数数量，也可能带来隐式正则化效果，迫使头学到更通用的表示。不过一般 Transformer 倾向让每个头独立，以分工捕捉不同关系，所以跨头共享较少在大模型中用，除非为了节省参数做小模型蒸馏等。</p><p><strong>那么==共享参数如何帮助长序列呢？</strong> 主要在于<strong>降低内存占用</strong>和<strong>允许用更多层来处理扩展序列</strong>。==比如，如果我们想通过增加层数来让模型看更长范围的信息，但参数量又受限，那共享可以让我们加层而参数不增加太多，从而持久“传递”上下文信息。同时，共享后的模型其实有一种循环特性，可以无限重复应用已学到的模式来处理任意长输入（这有点像 RNN 循环神经网络可以处理任意长度序列）。尽管 Transformer 不是天然循环的，但共享参数的 Transformer 在推理时理论上可以不断迭代处理延长的序列（不过实践中会受限于位置编码等）。</p><p>总的来说，跨层共享在 LLM 中不是主流策略，因为现在硬件允许的参数量还在不断提高，大家更倾向不惜参数直推性能。但在资源有限环境或移动端模型上，参数共享是压缩模型的有效方法。对于长序列任务，如果能用共享来构造一个“循环 Transformer”，或许能让模型具备类似无限长记忆的能力，未来可能有进一步研究。</p><p><strong>类比</strong>：参数共享有点像用同一个专家反复咨询，而不是每个阶段换新专家。比如你要一个连续的分析过程，传统 Transformer 相当于每个步骤请一个新顾问，各司其职。而共享参数意味着其实是同一个顾问在不同步骤多次参与。他对前后步骤的理解是一致的，可能更擅长长期推理。对于长文本，这位顾问因为反复阅览可能比一群各看一段的顾问更连贯。当然，每层独立也有好处：不同层可以专注不同抽象级别特征。因此实际中要不要共享，需要权衡表达力 vs. 参数高效。</p><h3 id="_2-3-4-位置外推与内插" tabindex="-1"><a class="header-anchor" href="#_2-3-4-位置外推与内插"><span>2.3.4 位置外推与内插</span></a></h3><p>即使我们采用高效注意力和记忆，使模型能够处理比原始长度更长的文本，但仍有一个限制因素：<strong>位置编码</strong>。Transformer 在训练时对每个序列中的位置都会形成某种位置表示。如果训练时只见过最多 n 长度，那么第 n+1, n+2…这些位置模型没学过，它可能不知道如何处理。这就需要<strong>外推(extrapolation)技术，使模型可以泛化到比训练更长</strong>的序列。相反，如果我们想在一个模型上微调到更短的序列，也涉及<strong>内插(interpolation)</strong> 概念，不过主要挑战是外推。</p><p>为了解决这问题，人们提出几种改进：</p><ul><li><strong>相对位置编码(Relative Position)</strong>：与其给每个绝对位置一个向量，不如让模型学会根据相对距离来判断相关性。这样模型不关心“第 1000 位”还是“第 2000 位”，而注重“相隔多少”。Transformer-XL 用的是一种相对位置方案。相对位置编码能一定程度上泛化到更长序列，因为无论多长，只要距离模式相似，模型就能处理。</li><li><strong>连续/坐标形式的位置表示</strong>：像 RoPE(旋转位置编码)把位置作为一个连续角度相位插入注意力计算。这种方式可以<strong>数学上</strong>外推——超出训练长度的角度也是定义良好的，从而模型可以推理出更长位置的表示。实践表明 RoPE 使模型在加倍长度时性能衰减较缓。还有 ALiBi（Attention with Linear Bias）方法，更是干脆不给超长位置限制，让注意力对远距离衰减但不会完全看不到，可以自然扩展长度。</li><li><strong>位置插值</strong>：OpenAI 在给 GPT-2 扩展上下文时用了一招，叫做<strong>缩放插值</strong>：假设原模型训练长 2048，如今想扩展到 4096，可以把输入第 4096 位置按照比例映射回 2048 的位置上（即位置索引乘以 2048/4096），这样让新位置的 embedding 用接近模型已知范围的 embedding。然后再微调模型少许步长，让它适应这种“压缩过的”位置编码空间。结果 GPT-2 就能用 4096 长度。这个属于经验性办法，但效果还不错，启示是很多位置编码可以通过线性变换做<strong>内插</strong>或<strong>外推</strong>调整。</li><li><strong>分段位置编码</strong>：还有种折中方式，超过一定长度后位置编码重新从 0 开始（比如每 512 token 作为一段，各段内位置重复使用编码）。模型可以学到段间如何衔接。不过如果直接重复可能混淆，需要结合段标识一起给模型。这个方法有限但简单，有的长文模型会这么处理比如将小说每章独立编码之类。</li></ul><p>总之，位置编码的改进让模型<strong>跳出训练时的长度限制</strong>。理论上，只要给模型合适的位置信号，它可以处理任意长文本——计算量和内存允许的话。现实中，例如 GPT-4 从原来的 8K 拓展到 32K 上下文，就用了新的位置编码（据说类似 RoPE）并加以微调，使得模型真正能用上更长输入，而不仅仅是能接受长输入却不懂得利用。</p><p><strong>类比</strong>：想象你平时练习只背过 100 行诗，现在突然让你背 200 行。普通 Transformer 就懵了，因为第 101 行以后它完全没概念。而如果用了好的位置编码，就像你学会了一种<strong>通用编号法</strong>或者节奏规律，可以把更长的诗也定位进去。比如相对位置法相当于你不记第几行，只记隔壁行是什么关系，这不管多少行都适用；RoPE 相当于给每行标一个角度，角度会顺延增加，超过之前范围照样延伸。这样，当出现第 150 行，你虽然训练时没见过，但你知道第 150 行和第 149 行是连续的，所以不至于手足无措。这让模型的“时间感”更具泛化性。</p><h3 id="_2-3-5-评述" tabindex="-1"><a class="header-anchor" href="#_2-3-5-评述"><span>2.3.5 评述</span></a></h3><p>长序列建模是让 LLM 更实用、更聪明的重要方向。例如，处理长文档让 AI 能阅读报告、法律合同；长对话上下文让聊天机器人记住先前细节、交流连贯；还有像视频、传感器数据等长时序也可用类似思想处理。当然，仍然没有哪种方案能完美支持任意长序列且性能与短序列一样好——目前都是在效率和效果间折中。<strong>Transformer 的瓶颈</strong>在长距离依赖上依然存在，但通过上述各种创新，我们已经部分突破了限制。</p><p>作为学习者，不必穷举每个方法的公式细节，更重要是理解 <strong>为什么需要这些方法</strong>：因为注意力机制的二次方复杂度，因为模型上下文窗口硬限制，因为位置编码泛化性差。解决思路也万变不离其宗：<strong>减少计算（稀疏/近似）</strong>，<strong>分段处理（缓存/记忆）</strong>，<strong>重复利用（共享）</strong>，<strong>改良表示（位置编码）</strong>。未来的模型也许会综合多种手段，例如一个模型在架构上用稀疏注意力、在推理时用记忆缓存、在训练中用了 RoPE 编码…… 目的是一个：能在合理资源内处理<strong>更多信息</strong>。</p><p><strong>现实展望</strong>：Anthropic 已经推出支持 10 万 token 上下文的 Claude 2 号模型，可以看作长序列建模的成功示范；OpenAI 等也在研究如何让模型接入外部知识库，这是另一种形式的长信息处理。随着硬件进步，我们或许终有一天可以有模型读完整部百科全书后一气回答问题。那将打开 AI 理解世界的新维度。但在那之前，我们还需要不断改进算法让模型高效利用每一点上下文，因为计算资源永远有限而人类想处理的信息无限多。</p><h2 id="_2-4-总结" tabindex="-1"><a class="header-anchor" href="#_2-4-总结"><span>2.4 总结</span></a></h2><ul><li><strong>大型语言模型(LLM)</strong> 是通过在海量语料上的预训练获得的一种强大生成模型，它能基于前文上下文连续地产生高质量文本 。LLM 多采用仅解码器 Transformer 架构，具备惊人的语言理解和生成能力。</li><li><strong>训练 LLM</strong>需要庞大的数据和算力支撑。通过最大化训练语料的概率（最小化预测损失），模型逐渐学会语言规律。训练如此巨大的模型往往需要借助==<strong>分布式并行</strong>技术（数据并行、模型并行、张量并行等）==来完成  。大量工程上的改进（如前置 LayerNorm、GeLU 激活、移除偏置、改进位置编码等）确保了超大模型训练的稳定和高效  。</li><li><strong>微调 LLM</strong>仍然重要，包括指令微调和领域微调，让模型更符合特定任务需求或人类偏好。特别地，<strong>对齐</strong>成为焦点，通过有监督微调和 RLHF 等方式，引导模型遵守人类意图和价值观  。经过对齐的模型更安全可靠，能拒绝不良请求并减少胡编乱造。</li><li><strong>提示(prompt)</strong> 是使用 LLM 的艺术。一条精心设计的提示可以让模型完成复杂任务而无需额外训练。掌握编写清晰指令、提供示例和上下文的提示技巧，能够最大限度发挥 LLM 能力。</li><li>==为了<strong>扩展上下文长度</strong>，研究者开发了多种方法：<strong>高效注意力</strong>降低长序列计算开销，<strong>缓存和记忆</strong>机制允许模型跨段保持信息，<strong>参数共享</strong>提高模型长程泛化能力，<strong>改进位置编码</strong>使模型可以泛化到超过训练长度的新位置 。==这些技术结合起来，让最新 LLM 能够处理过去望尘莫及的大段文本输入。</li><li><strong>缩放定律</strong>告诉我们增大模型和数据会持续改进性能，但收益递减 。合理平衡参数规模和训练数据至关重要（例如 Chinchilla 策略）。展望未来，如何以更优效率获得更强能力，将是持续的挑战。</li><li>综合来说，大模型时代的 NLP 范式已经形成：<strong>大数据预训练通用模型 -&gt; 微调/对齐模型 -&gt; 提示完成任务</strong> 。第一、二章内容奠定了理解这一范式的基础。接下来，进一步的章节将深入提示工程、高级对齐技巧和高效推理方法，帮助我们更好地应用和优化大型语言模型。</li></ul></div><!----><!----><!----></div></main><footer class="vp-doc-footer" data-v-15e751aa data-v-ed2b2ef3><!--[--><!--]--><!----><!----><nav class="prev-next" data-v-ed2b2ef3><div class="pager" data-v-ed2b2ef3><a class="vp-link link pager-link prev" href="/article/y3h97jhw/" data-v-ed2b2ef3><!--[--><span class="desc" data-v-ed2b2ef3>上一页</span><span class="title" data-v-ed2b2ef3>第3章 提示方法（Prompting）</span><!--]--><!----></a></div><div class="pager" data-v-ed2b2ef3><a class="vp-link link pager-link next" href="/article/vse0voeg/" data-v-ed2b2ef3><!--[--><span class="desc" data-v-ed2b2ef3>下一页</span><span class="title" data-v-ed2b2ef3>第1章 预训练</span><!--]--><!----></a></div></nav></footer><!----><!--[--><!--]--></div></div></div><!--[--><!--]--></div></div><!--]--><button style="display:none;" type="button" class="vp-back-to-top" aria-label="back to top" data-v-f306e958 data-v-a71e2a30><span class="percent" data-allow-mismatch data-v-a71e2a30>0%</span><span class="show icon vpi-back-to-top" data-v-a71e2a30></span><svg aria-hidden="true" data-v-a71e2a30><circle cx="50%" cy="50%" data-allow-mismatch style="stroke-dasharray:calc(0% - 12.566370614359172px) calc(314.1592653589793% - 12.566370614359172px);" data-v-a71e2a30></circle></svg></button><footer class="vp-footer" vp-footer data-v-f306e958 data-v-27191c5d><!--[--><div class="container" data-v-27191c5d><p class="message" data-v-27191c5d>Released under the <a href="https://github.com/Asxing/9zhe.tech/blob/master/LICENSE">MIT License</a>.</p><p class="copyright" data-v-27191c5d>Copyright © 2019-present <a href="https://github.com/Asxing">Asxing</a></p></div><!--]--></footer><!--[--><!--]--><!--]--></div><!----><!--]--><!--[--><!--]--><!--]--></div><script type="module" src="/assets/app-zSjPkmgN.js" defer></script></body></html>